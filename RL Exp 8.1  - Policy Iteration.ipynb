{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6c98be9-0c4a-4137-a591-389636c2407a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 16\n",
      "Number of actions: 4\n",
      "\n",
      "Optimal Policy (best action at each state):\n",
      "[1 2 1 0 1 0 1 0 2 1 1 0 0 2 2 0]\n",
      "\n",
      "Optimal Value Function (long-term value of each state):\n",
      "[[0.59049 0.6561  0.729   0.6561 ]\n",
      " [0.6561  0.      0.81    0.     ]\n",
      " [0.729   0.81    0.9     0.     ]\n",
      " [0.      0.9     1.      0.     ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Step 1: Import required libraries\n",
    "# -------------------------------\n",
    "\n",
    "import numpy as np          # For numerical operations like arrays, max, argmax\n",
    "import gym                  # To create the FrozenLake environment\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Create the environment\n",
    "# -------------------------------\n",
    "\n",
    "# Create a 4x4 FrozenLake environment\n",
    "# is_slippery=False makes it deterministic (no random slips)\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False)\n",
    "\n",
    "# Print environment details\n",
    "print(\"Number of states:\", env.observation_space.n)  # Total possible states (16)\n",
    "print(\"Number of actions:\", env.action_space.n)      # Total possible actions (4)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Initialize policy & value function\n",
    "# -------------------------------\n",
    "\n",
    "# Policy: for each state, choose a random action initially\n",
    "policy = np.random.choice(env.action_space.n, size=env.observation_space.n)\n",
    "\n",
    "# Value function: start with zeros for all states\n",
    "value_table = np.zeros(env.observation_space.n)\n",
    "\n",
    "# Discount factor (gamma) to prioritize immediate vs. future rewards\n",
    "gamma = 0.9\n",
    "\n",
    "# Convergence threshold for policy evaluation\n",
    "theta = 1e-10\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Policy Evaluation\n",
    "# -------------------------------\n",
    "\n",
    "def policy_evaluation(policy, value_table, env, gamma=0.9, theta=1e-10):\n",
    "    \"\"\"\n",
    "    Evaluate the value function under the current policy.\n",
    "    Iteratively update state values until convergence.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        delta = 0  # Tracks maximum change in value during an iteration\n",
    "        # Loop through all states\n",
    "        for state in range(env.observation_space.n):\n",
    "            v = 0\n",
    "            action = policy[state]  # Action chosen by current policy\n",
    "            # Look at possible transitions for this (state, action)\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                # Bellman expectation equation\n",
    "                v += prob * (reward + gamma * value_table[next_state])\n",
    "            # Track max value change\n",
    "            delta = max(delta, abs(value_table[state] - v))\n",
    "            value_table[state] = v  # Update value of state\n",
    "        # Stop if values converge (small change)\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return value_table\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Policy Improvement\n",
    "# -------------------------------\n",
    "\n",
    "def policy_improvement(value_table, policy, env, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Improve the policy using the current value function.\n",
    "    \"\"\"\n",
    "    policy_stable = True\n",
    "    # Loop through all states\n",
    "    for state in range(env.observation_space.n):\n",
    "        old_action = policy[state]  # Current action\n",
    "        action_values = np.zeros(env.action_space.n)  # Store values of all actions\n",
    "        # Try all possible actions\n",
    "        for action in range(env.action_space.n):\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                action_values[action] += prob * (reward + gamma * value_table[next_state])\n",
    "        # Choose action with highest value\n",
    "        policy[state] = np.argmax(action_values)\n",
    "        # Check if policy changed\n",
    "        if old_action != policy[state]:\n",
    "            policy_stable = False\n",
    "    return policy, policy_stable\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Step 6: Policy Iteration\n",
    "# -------------------------------\n",
    "\n",
    "def policy_iteration(env, gamma=0.9, theta=1e-10):\n",
    "    \"\"\"\n",
    "    Run policy iteration algorithm.\n",
    "    Returns optimal policy and value function.\n",
    "    \"\"\"\n",
    "    # Initialize policy and value table\n",
    "    policy = np.random.choice(env.action_space.n, size=env.observation_space.n)\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "\n",
    "    while True:\n",
    "        # Step 1: Evaluate current policy\n",
    "        value_table = policy_evaluation(policy, value_table, env, gamma, theta)\n",
    "\n",
    "        # Step 2: Improve policy based on new values\n",
    "        policy, policy_stable = policy_improvement(value_table, policy, env, gamma)\n",
    "\n",
    "        # Stop if policy no longer changes\n",
    "        if policy_stable:\n",
    "            return policy, value_table\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Step 7: Run Policy Iteration\n",
    "# -------------------------------\n",
    "\n",
    "optimal_policy, optimal_value = policy_iteration(env, gamma, theta)\n",
    "\n",
    "print(\"\\nOptimal Policy (best action at each state):\")\n",
    "print(optimal_policy)\n",
    "\n",
    "print(\"\\nOptimal Value Function (long-term value of each state):\")\n",
    "print(optimal_value.reshape(4,4))  # Reshape into 4x4 grid for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c2452-43bb-4943-ac32-74c7c7bace14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
